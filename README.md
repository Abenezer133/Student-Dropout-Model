# ğŸ“ Student Dropout Risk Prediction

This is my first applied machine learning project.  
The idea was to see if simple student data can be used to predict who might be at risk of dropping out.  
I wanted to learn not only about models, but also about how to explain them.

---

## ğŸ“Œ Project Overview

- **Problem:** Many schools only notice dropout risk when itâ€™s already too late.  
- **Goal:** Build a small prototype model to flag students earlier.  
- **Approach:**  
  - Tried Logistic Regression (baseline)  
  - Tried XGBoost (more powerful)  
  - Used SHAP for interpretability (to see which features matter)  
  - Measured performance with F1-score and Precision@Top10%  

---

## âš™ï¸ Features Used

- `attendance_rate` â†’ percentage of classes attended  
- `avg_grade` â†’ average grade across assignments/exams  
- `late_submissions` â†’ number of late assignments  
- `lms_activity` â†’ engagement on the learning platform  

---

## ğŸ“Š Results

- **Logistic Regression (baseline):** F1 = **0.50**  
- **XGBoost (best):** F1 = **1.0** (on this small dataset â€” probably overfitting)  
- **Precision@Top10%:** 1.0 â†’ the top 10% most â€œat-riskâ€ students flagged by the model were truly at risk  
- **SHAP explainability:** attendance rate and average grade were the most important factors  

*(Since the dataset was small and simulated, the XGBoost results look â€œtoo perfect.â€ In real life with bigger data, results would be more realistic.)*  

ğŸ§  **What I Learned**

How to build and compare ML models (baseline vs advanced)

The importance of explainability in sensitive applications like education

Why overfitting happens and how to watch out for it

How to evaluate model performance beyond accuracy (e.g. F1, precision@top%)
---

## ğŸš€ How to Run

1. Open the notebook in **Google Colab** (this repo includes a link).  
2. Install dependencies:  
